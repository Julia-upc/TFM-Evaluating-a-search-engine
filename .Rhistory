sum(difference>3.18)/length(difference)
# Using package perm
library(perm)
permTS(height[1:10],height[11:20],method="exact.mc", alternative="greater")
# Example 2: Directed Reading Activities study (drp no me'l detecta!!!!!!!)
observed.diff<-mean(drp$drp[drp$g==0])-mean(drp$drp[drp$g==1])
new.drp<-numeric(0)
difference<-numeric(0)
nrep=10000
for(i in 1:nrep){
new.drp<-sample(drp$drp, size=44, replace=FALSE)
difference<-c(difference,mean(new.drp[1:21])-mean(new.drp[22:44]))
}
# one-sided test p-value=0.0126 (da esto)
sum(difference>observed.diff)/nrep
#two-sided test p-value=0.0249 (és el que dona)
sum(abs(difference)>observed.diff)/nrep
# Example 3: ANOVA with permutation test
library("faraway")
head(coagulation)
#observed results
ggplot(coagulation,aes(x=diet, y=coag))+geom_boxplot() #cada un dels blogs és un tipus de dieta
aov_results = lm(coag ~ diet, data = coagulation)
anova(aov_results)
obt.F = anova(aov_results)$"F value"[[1]] #test statistic
p_val  = anova(aov_results)$"Pr(>F)"[[1]]
nreps<-5000
samp.F <- numeric(nreps)
counter <- 0
for (i in 1:nreps){
new_groups<-sample(coagulation$diet)
new_model<-lm(coagulation$coag ~ new_groups, data = coagulation)
samp.F[i] <- anova(new_model)$"F value"[1]
if (samp.F[i] > obt.F) counter = counter + 1
}
p.value<-counter/nreps
p.value
# Example 4: another ANOVA with permutation test
# Ants and type of bread in Sandwiches
load("Data/SandwichAnts2.rda")
ggplot(SandwichAnts2,aes(x=Bread, y=Ants))+geom_boxplot()
aov_results = lm(Ants ~ Bread, data = SandwichAnts2)
anova(aov_results)
obt.F = anova(aov_results)$"F value"[[1]]
p_val  = anova(aov_results)$"Pr(>F)"[[1]]
nreps<-5000
samp.F <- numeric(nreps)
counter <- 0
for (i in 1:nreps){
new_groups<-sample(SandwichAnts2$Bread)
new_model<-lm(SandwichAnts2$Ants ~ new_groups, data = coagulation)
samp.F[i] <- anova(new_model)$"F value"[1]
if (samp.F[i] > obt.F) counter = counter + 1
}
p.value<-counter/nreps
p.value # the p value is not zero, it is that we have observed less than one in 1000. Never report a p value that is 1 or 0
# Example 5: coefficient of correlation
#load("Data/RestaurantTips.rda")
load("/home/julia/Escritorio/Statistical data analysis/perm.RData")
head(RestaurantTips)
ggplot(RestaurantTips, aes(y=Bill, x=PctTip))+geom_point()
obs.cor=cor(RestaurantTips$Bill, RestaurantTips$PctTip)
obs.cor
sampling.distr=replicate(10000,cor(sample(RestaurantTips$Bill),RestaurantTips$PctTip))
# p-value two-tailed test
sum(abs(sampling.distr)>=obs.cor)/10000
# p-value one tailed test (greater than)
sum(sampling.distr>=obs.cor)/10000
hist(sampling.distr)
# Example 6: paired data  # not described in the moodle pdf
# Moon data: agressive incidents under a full moon
# moon<-`EG16-14MOON`
moon_o=moon$aggmoon
other_o=moon$aggother
obs.difference=mean(moon$aggmoon)-mean(moon$aggother)
moon_s<-vector()
other_s<-vector()
difference<-numeric(0)
for (i in 1:10000){
for (j in 1:15) {
p=runif(1)
if (p>=0.5){
moon_s[j]<-moon_o[j]
other_s[j]<-other_o[j]
} else { moon_s[j]<-other_o[j]
other_s[j]<-moon_o[j]
}
}
difference=c(difference,mean(moon_s)-mean(other_s))
}
# p-value: none of these resamples produces a difference as large as the
# observed difference 2.43
sum(difference>obs.difference)
# The p-values is less than 1 in 10000, p-value < 0.0001
# The p-values is less than 1 in 10000, p-value < 0.0001
# NOt every time you can design a permutation test that fits your problem/design!!!!!
# The p-values is less than 1 in 10000, p-value < 0.0001
# NOt every time you can design a permutation test that fits your problem/design!!!!!
#loading libraries
library("tidyverse")
library("tseries")
library("car") # to use Levene's test
library("coin")
library("vcd") #visualization of categorical data
library("rcompanion") # to use function plotNormalHistogram()
library("BSDA")
library("MASS")
####### Data ########
load("two_groups.RData")
#####################
# INDEPENDENT SAMPLES
####################
# t-test paired=FALSE
Input = ("
Instructor       Student  Sodium
'Brendon Small'  a        1200
'Brendon Small'  b        1400
'Brendon Small'  c        1350
'Brendon Small'  d         950
'Brendon Small'  e        1400
'Brendon Small'  f        1150
'Brendon Small'  g        1300
'Brendon Small'  h        1325
'Brendon Small'  i        1425
'Brendon Small'  j        1500
'Brendon Small'  k        1250
'Brendon Small'  l        1150
'Brendon Small'  m         950
'Brendon Small'  n        1150
'Brendon Small'  o        1600
'Brendon Small'  p        1300
'Brendon Small'  q        1050
'Brendon Small'  r        1300
'Brendon Small'  s        1700
'Brendon Small'  t        1300
'Coach McGuirk'  u        1100
'Coach McGuirk'  v        1200
'Coach McGuirk'  w        1250
'Coach McGuirk'  x        1050
'Coach McGuirk'  y        1200
'Coach McGuirk'  z        1250
'Coach McGuirk'  aa       1350
'Coach McGuirk'  ab       1350
'Coach McGuirk'  ac       1325
'Coach McGuirk'  ad       1525
'Coach McGuirk'  ae       1225
'Coach McGuirk'  af       1125
'Coach McGuirk'  ag       1000
'Coach McGuirk'  ah       1125
'Coach McGuirk'  ai       1400
'Coach McGuirk'  aj       1200
'Coach McGuirk'  ak       1150
'Coach McGuirk'  al       1400
'Coach McGuirk'  am       1500
'Coach McGuirk'  an       1200
")
Data = read.table(textConnection(Input),header=TRUE)
str(Data)
head(Data)
rm(Input)
Data$Instructor<- as.factor(Data$Instructor)
# checking normality, needed to apply t-procedures
jarque.bera.test(Data$Sodium[Data$Instructor=="Brendon Small"])
jarque.bera.test(Data$Sodium[Data$Instructor=="Coach McGuirk"])
# test to compare the variances of two (or more) groups (library car).
# Bartlett's test requires the data to be normally distributed
bartlett.test(Sodium~Instructor, data=Data)
# Levene's test is a robust alternative to the Bartlett's test that is less sensitive to departures from
# normality
leveneTest(Data$Sodium~Data$Instructor)
# Fligner's test is a non-parametric test based on ranks, very robust against departures from normality
# (again, to compare the variances of two groups)
fligner.test(Sodium~Instructor, data=Data)
# Testing equality of means for the two groups
t.test(Sodium ~ Instructor,
data = Data)
ggplot(Data, aes(x=Instructor, y=Sodium))+geom_boxplot()
t.test(phone$Cell.phone,phone$Control,var.equal = TRUE)
# 2.t-test Reaction times in the data frame "phone"
phone=read.csv(file="Data/CellPhoneReactionTime_wideformat_t-test_notpaired.csv", header=TRUE)
t.test(phone$Cell.phone,phone$Control,var.equal = TRUE)
# use var.equal=FALSE, the default option, if one sample standard deviation is more than double the other one.
t.test(phone$Cell.phone,phone$Control)
# Some data manipulation to check the standard deviations of both groups.
phone_2<-c(phone$Cell.phone,phone$Control)
group<-c(rep("Phone",32),rep("Control",32))
phone.df<-data.frame(phone_2,group)
ggplot(phone.df, aes(x=group, y=phone_2)) + geom_boxplot()
phone.df %>% group_by(group) %>% summarise(n=n(), st.dev=sd(phone_2))
Input =("
Speaker  Likert
Instr.1      3
Instr.1      5
Instr.1      4
Instr.1      4
Instr.1      4
Instr.1      4
Instr.1      4
Instr.1      4
Instr.1      5
Instr.1      5
Instr.2    2
Instr.2    4
Instr.2    2
Instr.2    2
Instr.2    1
Instr.2    2
Instr.2    3
Instr.2    2
Instr.2    2
Instr.2    3
")
Data = read.table(textConnection(Input),header=TRUE)
rm(Input)
str(Data)
summary(Data)
Data$Likert.f = factor(Data$Likert,
ordered = TRUE)
wilcox.test(Likert ~ Speaker,
data=Data)
wilcox.test(Likert ~ Speaker,
data=Data, exact=FALSE)
# Second example Wilcoxon test: independent samples
# In the data frame column mpg of the data set mtcars, there are gas mileage data of various 1974 U.S. automobiles.
data(mtcars)
wilcox.test(mpg ~ am, data=mtcars, exact=FALSE)
ants=read.csv(file="Data/ants.csv", header=TRUE)
ants$place=factor(ants$place)
# effectively, generating all possible combinations
oneway_test(colonies~place,data=ants,distribution="exact")
# approximate p-values (taking many random samples)
oneway_test(colonies~place,data=ants,distribution=approximate(nresample = 9999))
ants=read.csv(file="Data/ants.csv", header=TRUE)
ants$place=factor(ants$place)
# effectively, generating all possible combinations
oneway_test(colonies~place,data=ants,distribution="exact")
# approximate p-values (taking many random samples)
oneway_test(colonies~place,data=ants,distribution=approximate(nresample = 9999))
# Categorical response, two groups: chi-squared test of independence
ski.freq=data.frame(expand.grid(Treatment=c("VitC", "Placebo"), Result=c("Cold", "NoCold")), freq=c(17,31,122,109))
ski.table=xtabs(freq~Treatment+Result, data=ski.freq)
ski.table
chisq.test(ski.table) # it requires some conditions on table counts
chisq.test(ski.table, simulate.p.value = TRUE, B=20000) # permutation test
mosaic(ski.table,gp=shading_max, split_vertical=TRUE)
# another example a 2x3 contingency table
data(Arthritis)
head(Arthritis)
str(Arthritis)
table1=with(Arthritis, table(Treatment, Improved))
table1
chisq.test(table1)
mosaic(table1,gp=shading_max, split_vertical=TRUE)
# let's introduce the gender in this brief analysis
mosaic(~Sex+Improved+Treatment, data=Arthritis,shade=TRUE)
tabla=xtabs(~Treatment+Improved+Sex, data=Arthritis)
# For females
chisq.test(tabla[,,1])
chisq.test(tabla[,,1], simulate.p.value = TRUE)
mosaic(tabla[,,1], gp=shading_max)
# For males
chisq.test(tabla[,,2])
chisq.test(tabla[,,2], simulate.p.value = TRUE)
mosaic(tabla[,,2], gp=shading_max)
# another example Happiness and Family Income a 3x3 contingency table
happiness=data.frame(expand.grid(Income=c("Above average", "Average", "Below Average"),
Happiness=c("Not Too Happy", "Pretty Happy", "Very Happy")),
freq=c(29,83,104,178,494,314,135,277,119))
happiness_table=xtabs(freq~Income+Happiness, data=happiness)
chisq.test(happiness_table)
mosaic(happiness_table,gp=shading_max, split_vertical=TRUE)
#####################
# DEPENDENT SAMPLES #
#####################
# t.test, paired=TRUE, data set phone_dep
# phone_dep=read.csv(file="Data/ReactionTime_wide_t-test_paired.csv", header=TRUE)
head(phone_dep)
#####################
# DEPENDENT SAMPLES #
#####################
# t.test, paired=TRUE, data set phone_dep
phone_dep=read.csv(file="Data/ReactionTime_wide_t-test_paired.csv", header=TRUE)
head(phone_dep)
# the following function is library rcompanion
plotNormalHistogram(phone_dep$Diff,xlab="Difference")
t.test(phone_dep$Diff)
factor=c(rep("no",32),rep("yes",32))
data=c(phone_dep$No,phone_dep$Yes)
phone_1=data.frame(factor, data)
t.test(data~factor, data=phone_1, paired=TRUE) # dependent samples
Input =("
Speaker  Time  Student  Likert
Instr.1      1     a        1
Instr.1      1     b        4
Instr.1      1     c        3
Instr.1      1     d        3
Instr.1      1     e        3
Instr.1      1     f        3
Instr.1      1     g        4
Instr.1      1     h        3
Instr.1      1     i        3
Instr.1      1     j        3
Instr.1      2     a        4
Instr.1      2     b        5
Instr.1      2     c        4
Instr.1      2     d        5
Instr.1      2     e        4
Instr.1      2     f        5
Instr.1      2     g        3
Instr.1      2     h        4
Instr.1      2     i        3
Instr.1      2     j        4
")
Data = read.table(textConnection(Input),header=TRUE)
head(Data)
# remove unnecesary data
rm(Input)
Time.1 = Data$Likert [Data$Time == 1]
Time.2 = Data$Likert [Data$Time == 2]
library(BSDA)
SIGN.test(x = Time.1,
y = Time.2,
alternative = "two.sided",
conf.level = 0.95)
# sign test: What do most students spend more time doing: browsing the internet or watching TV?
# georgia_students<-read.csv(file="Data/georgia_student_survey.csv", header=TRUE)
# Data set georgia_students
SIGN.test(x=georgia_students$WatchTV, y=georgia_students$BrowseInternet, alternative="greater")
# This is equivalent to:
# 54 different pairs of response
sum(georgia_students$WatchTV!=georgia_students$BrowseInternet)
sum(georgia_students$WatchTV>georgia_students$BrowseInternet)
binom.test(35,54,alternative="greater")
# In the built-in data set named immer, the barley yield in years 1931 and 1932 of the same field are recorded.
# The yield data are presented in the data frame columns Y1 and Y2.
#
library(MASS)         # load the MASS package
head(immer)
wilcox.test(immer$Y1, immer$Y2, paired=TRUE, exact=FALSE, alternative="greater")
plot(immer$Y1, jitter(immer$Y2),   # jitter offsets points so you can see them all
pch = 16,                 # shape of points
cex = 1.0,                # size of poin
xlim=c(50, 200),          # limits of x axis
ylim=c(50, 200),          # limits of y axis
xlab="Year 1",
ylab="Year 2"
)
abline(0,1, col="blue", lwd=2)
# Revisiting the Watching TV - Browsing the internet example
wilcox.test(georgia_students$WatchTV,georgia_students$BrowseInternet,paired=TRUE, exact=FALSE, alternative="greater")
data <- matrix(c(794, 86, 150, 570),
nrow = 2,
dimnames = list("Time 1" = c("Approved", "Disapproved"),
"Time 2" = c("Approved", "Disapproved")))
mcnemar.test(data, correct=FALSE)
# Speech recognition system
speech <- matrix(c(1921, 16, 58, 5),
nrow = 2,
dimnames = list("Method 1" = c("Correct", "Incorrect"),
"Method 2" = c("Correct", "Incorrect")))
mcnemar.test(speech)
# t_facebook<-read.csv(file="Data/timeFB.csv", header=TRUE)
t_facebook$class<-as.factor(t_facebook$class)
t_facebook %>% group_by(class) %>% summarize(n=n(), mean=mean(FBtime), st.dev=sd(FBtime))
ggplot(t_facebook, aes(x=class, y=FBtime )) + geom_boxplot()
ggplot(t_facebook, aes(x=class, y=log(FBtime) )) + geom_boxplot()
summary(aov(log(t_facebook$FBtime+1)~t_facebook$class))
# Another ANOVA example with the dataset mtcars. Response variable mpg, groups given by cylinder
data(mtcars)
mtcars$cyl<-as.factor(mtcars$cyl)
ggplot(mtcars, aes(cyl, mpg, group=cyl)) + geom_boxplot()
mtcars %>% group_by(cyl) %>% summarize(n=n(), mean=mean(mpg), st.d=sd(mpg))
# Checking normality of each group. Small sample size, use Shapiro-wilks test
data_split <- split(mtcars$mpg, as.factor(mtcars$cyl))
for (i in 1:length(data_split)){
data_split.i <- as.vector(data_split[i])
print(shapiro.test(data_split.i[[1]]))
}
# Checking equality of variances in each group
leveneTest(mtcars$mpg,mtcars$cyl,center=mean)
# Trasforming the response variable
mtcars<-mtcars %>% mutate(log_mpg=log(mpg))
mtcars$cyl<-as.factor(mtcars$cyl)
ggplot(mtcars, aes(cyl, log_mpg, group=cyl)) + geom_boxplot()
data_split <- split(mtcars$log_mpg, as.factor(mtcars$cyl))
for (i in 1:length(data_split)){
data_split.i <- as.vector(data_split[i])
print(shapiro.test(data_split.i[[1]]))
}
leveneTest(mtcars$log_mpg,mtcars$cyl,center=mean)
model.aov <- aov(log_mpg~cyl, data=mtcars)
summary(model.aov)
# Post Hoc comparisons
TukeyHSD(model.aov)
plot(TukeyHSD(model.aov), las=1)
# Kruskal-Wallis test
# Frequent dating and College GPA. Entering data:
data<-c(1.75, 3.15, 3.50, 3.68, 2.00, 3.20, 3.44, 3.50, 3.60, 3.71, 3.80, 2.40, 2.95, 3.40, 3.67, 3.70, 4.00)
group<-c(rep("Rare",4), rep("Occasional",7), rep("Regular",6) )
datos<-data.frame(data,group)
kruskal.test(datos$data~datos$group)
datos$group<-as.factor(datos$group)
ggplot(datos, aes(x=group, y=data)) + geom_dotplot(binwidth=0.09,binaxis='y')
# For the facebook data earlier, as it didn't meet ANOVA requirements,
# is better to perform the Kruskal-Wallis test
kruskal.test(t_facebook$FBtime~t_facebook$class)
model_lm=lm(as.matrix(dogs)~1)
dogs=read.table("dogs.dat")
dogs=read.table("dogs.dat")
dogs=read.table("dogs.dat")
model_lm=lm(as.matrix(dogs)~1)
treatment<-factor(c("Tr1","Tr2","Tr3","Tr4"))
library("car")
anova_pareado <- Anova(model_lm, idata = data.frame(treatment),
idesign = ~ treatment, type = "III")
summary(anova_pareado)
datos_tabla_larga <- gather(data = dogs, key = "V", value = "dat", 1:4)
ggplot(data = datos_tabla_larga, aes(x = V, y = dat, colour = V)) +
geom_boxplot() +
theme_bw() +
theme(legend.position = "none")
pairwise.t.test(x = datos_tabla_larga$dat, g = datos_tabla_larga$V,
p.adjust.method = "holm", paired = TRUE, alternative = "two.sided")
#Friedman rank sum test
friedman.test(as.matrix(dogs))
# Cochran's Q test: Dependent samples, categorical response, more than 2 groups
library(coin)
Response <- c(1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1)
Subject <- factor(c(1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 10))
Canal <- factor(rep(c("initial", "ad", "internet"), 10))
data <- data.frame(Subject, Canal, Response)
symmetry_test(Response ~ factor(Canal) | factor(Subject), data = data, teststat = "quad")
library(boot)
library(moments) # to compute skewness
library(tseries) # to use Jarque-Bera test for normality
# Data facebook_1, 21 observations of time spent looking at a given Facebook profile
# Histogram and normal probability plot of the data. The data are skewed to the right.
# Given the relatively small sample size, we have some concerns about using t procedures with
# this data set
# Load data sets
load("boots.RData")
load("~/Escritorio/Statistical data analysis/boots.RData")
# Data set facebook_1
hist(facebook_1$Time)
qqnorm(facebook_1$Time)
qqline(facebook_1$Time)
shapiro.test(facebook_1$Time)
# Bootstrap distribution of the mean time looking at a given Facebook profile
obs.mean<-mean(facebook_1$Time)
fcb.boot.mean<- numeric(0)
fcb.boot.var<- numeric(0)
set.seed(12345)
for (i in 1:10000) {
temp<- sample(facebook_1$Time,size=length(facebook_1$Time), replace=TRUE)
fcb.boot.mean<-c(fcb.boot.mean, mean(temp))
fcb.boot.var<-c(fcb.boot.var, var(temp))
}
hist(fcb.boot.mean)
# Bootstrap t confidence interval (95%)
t=qt(0.975,20)
t
#lower bound
obs.mean-t*sd(fcb.boot.mean)
# upper bound
obs.mean+t*sd(fcb.boot.mean)
# Percentile confidence interval
quantile(fcb.boot.mean, probs=c(0.025, 0.975))
# Pivot method
# Lower limit
2*obs.mean-quantile(fcb.boot.mean, probs=c(0.975))
# Upper limit
2*obs.mean-quantile(fcb.boot.mean, probs=c(0.025))
# Bootstrap within bootstrap
obs.mean<-median(facebook_1$Time)
fcb.boot.mean<- numeric(0)
subsample.mean<-vector() #vector vacio para ir rellenando
t.asterisk<-numeric(0)
for (i in 1:10000) {
temp<- sample(facebook_1$Time,size=length(facebook_1$Time), replace=TRUE)
fcb.boot.mean<-c(fcb.boot.mean, mean(temp))
for (j in 1:100){
temp_2<-sample(temp, size=length(temp), replace=TRUE)
subsample.mean[j]=mean(temp_2)
}
t.asterisk<-c(t.asterisk, (mean(temp)-obs.mean)/sd(subsample.mean) )
}
q=quantile(t.asterisk, probs=c(0.025,0.975), na.rm=TRUE)
q
install.packages('tidyverse')
library('tidyverse')
install.packages("tidyverse")
library(tidyverse) #data manipulation
install.packages("tidyverse")
library(tidyverse) #data manipulation
library(GGally) # nice scatterplot matrix
library(FactoMineR) # PCA computation
library(factoextra) # nice plotting for PCA objects
library(missMDA) # to determine number of PC's through crossvalidation
library(gridExtra) # to build grid of plots
#check you have the correct data set
cereals <- read.table("cerealdata.txt", header=TRUE, as.is=TRUE, na.strings="-1")
#check you have the correct data set
cereals <- read.table("cerealdata.txt", header=TRUE, as.is=TRUE, na.strings="-1")
#check you have the correct data set
cereals <- read.table("cereal.txt", header=TRUE, as.is=TRUE, na.strings="-1")
#check you have the correct data set
cereals <- read.table("cerealdata.txt", header=TRUE, as.is=TRUE, na.strings="-1")
#check you have the correct data set
cereals <- read.table("cerealdata.txt", header=TRUE, as.is=TRUE, na.strings="-1")
#check you have the correct data set
cereals <- read.table("cerealdata.txt", header=TRUE, as.is=TRUE, na.strings="-1")
setwd("~/Escritorio/TFM/TFM-Evaluating-a-search-engine")
